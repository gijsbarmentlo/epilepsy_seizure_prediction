{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exceptional-crack",
   "metadata": {},
   "source": [
    "# Epileptic seizure prediction based on intracranial electroencephalography (EEG) data\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "## Summary\n",
    "\n",
    "### I - Feature generation\n",
    "\n",
    "<ol>\n",
    "<li> <strong><em> Setup </em></strong>  </li>\n",
    "    <ol>\n",
    "        <li> Imports </li>\n",
    "        <li> Parameters </li>\n",
    "        <li> Logging </li>\n",
    "    </ol>\n",
    "<br>\n",
    "\n",
    "<li> <strong><em> Helper functions </em></strong> </li>\n",
    "    <ol>\n",
    "        <li> Reading data from .mat files </li>\n",
    "        <li> Timeseries features </li>\n",
    "        <li> Entropy features </li>\n",
    "    </ol>    \n",
    "<br>\n",
    "\n",
    "<li> <strong><em> Feature generation loop </em></strong> </li>\n",
    "    <ol>\n",
    "        <li> Feature generation function </li>\n",
    "        <li> Feature generation in loop over patients </li>\n",
    "    </ol>\n",
    "<br>\n",
    "\n",
    "<li> <strong><em> EDA of the generated dataset </em></strong> </li>\n",
    "    <ol>\n",
    "        <li> zero values and NANs </li>\n",
    "        <li> Feature distibutions </li>\n",
    "    </ol>\n",
    "</ol>\n",
    "\n",
    "\n",
    "### II - Machine learning model\n",
    "\n",
    "<ol>\n",
    "<li> <strong><em> Setup </em></strong> </li>\n",
    "    <ol>\n",
    "        <li> Imports </li>\n",
    "        <li> Metrics function </li>\n",
    "        <li> Create test and train datasets  </li>\n",
    "    </ol>\n",
    "<br>\n",
    "\n",
    "<li> <strong><em> Model </em></strong> </li>\n",
    "    <ol>\n",
    "        <li> Choosing type of model </li>\n",
    "        <li> Hyperparameter tuning with genetic algo </li>\n",
    "        <li> ROC Curve and choosing threshold </li>\n",
    "    </ol>  \n",
    "</ol>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-lebanon",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-claim",
   "metadata": {},
   "source": [
    "Epilepsy is one of the most common brain disorders, [according to the CDC](http://www.ur.ac.rw) 1.2% of the population has active epilepsy. About 40% of them have seizures that are not controlled by medication ([Kwan and Brodie 2000](https://pubmed.ncbi.nlm.nih.gov/11034869/)). The unpredictable nature of epileptic seizures can have a significant impact on their lives, making certain common activities such as driving or swimming potentially life-threatening. Being able to predict the onset of seizures could drastically improve the quality of life of those affected.  \n",
    "The point of this work is to contribute to the [collective effort](https://academic.oup.com/brain/article/141/9/2619/5066003) guided by [the Epilepsy Ecosystem](epilepsyecosystem.org).  \n",
    "A classifier will be designed and trained to maximise the AUC for 3 patients. It will be trained on features generated from the 16 channel EEG data from the NeuroVista trials for these same patients and then evaluated independently by the Epilepsy Ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-accident",
   "metadata": {},
   "source": [
    "# I - Feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-mirror",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-court",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "parental-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import numpy as np\n",
    "from scipy.integrate import simps\n",
    "import scipy.io as sio\n",
    "import scipy.stats\n",
    "import scipy.signal\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from datetime import datetime, date\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-museum",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "spatial-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'C:/Users/gijsb/OneDrive/Documents/epilepsy_neurovista_data/'\n",
    "TRAIN_PATHS = [f'Pat{i}Train' for i in [1, 2, 3]]\n",
    "TEST_PATHS = [f'Pat{i}Test' for i in [1, 2, 3]]\n",
    "FEATURE_SAVE_PATH = DATA_PATH # Path where output feature arrays will be saved\n",
    "\n",
    "SAMPLING_FREQUENCY = 400\n",
    "DOWNSAMPLING_RATIO = 5\n",
    "CHANNELS = range(0,16)\n",
    "BANDS = [0.1,1,4,8,12,30,70]\n",
    "HIGHRES_BANDS = [0.1,1,4,8,12,30,70,180]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-embassy",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "supposed-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "#today_string = str(datetime.now())[0:19].replace('-', '_').replace(':', '_').replace(' ', '_')\n",
    "now = datetime.now()\n",
    "today_string = now.strftime(\"%d_%m_%Y__%H_%M_%S\")\n",
    "log_filename = f'feature_generation_log_{today_string}.log'\n",
    "logging.basicConfig(level=logging.DEBUG, \n",
    "                    filename=log_filename, \n",
    "                    format='%(asctime)s.%(msecs)03d %(levelname)s {%(module)s} [%(funcName)s] %(message)s', \n",
    "                    datefmt='%Y-%m-%d,%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-cisco",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-specification",
   "metadata": {},
   "source": [
    "#### Reading .mat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "complimentary-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mat(mat_file_path):\n",
    "    \"\"\"\n",
    "    input : filepath string\n",
    "    output : numpy array, returns zeros if it cannot read the file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = (sio.loadmat(mat_file_path)['data']).T\n",
    "        logging.debug('data loaded')\n",
    "        return data\n",
    "\n",
    "    except Exception:\n",
    "        warnings.warn(f'error reading .mat file {mat_file_path}')\n",
    "        return np.zeros((16, 240000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-recovery",
   "metadata": {},
   "source": [
    "#### Features on timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "integrated-characteristic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_crossings(data):\n",
    "    pos = data > 0\n",
    "    return (pos[:-1] & ~pos[1:]).nonzero()[0].shape[0]\n",
    "\n",
    "\n",
    "def band_energy(f, psd, low_f, high_f):\n",
    "    # Find intersecting values in frequency vector\n",
    "    idx_delta = np.logical_and(f >= low_f, f <= high_f)\n",
    "    # The frequency resolution is the size of each frequency bin\n",
    "    freq_res = f[1] - f[0]\n",
    "    # Compute the absolute power by approximating the integral\n",
    "    return simps(psd[idx_delta], dx=freq_res)\n",
    "\n",
    "\n",
    "def total_energy(segment_downsampled):\n",
    "    # From litterature the lowest frequencies of interest in a EEG is 0.5Hz so we need to keep our resolution at 0.25Hz hence a 4 second window cf.Nyquist\n",
    "    window = SAMPLING_FREQUENCY / DOWNSAMPLING_RATIO * 4\n",
    "    f, psd = scipy.signal.welch(\n",
    "        segment_downsampled, fs=SAMPLING_FREQUENCY/DOWNSAMPLING_RATIO, nperseg=window)\n",
    "    return psd.sum()\n",
    "\n",
    "\n",
    "def highres_total_energy(segment):\n",
    "    window = SAMPLING_FREQUENCY * 4\n",
    "    f, psd = scipy.signal.wel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-chancellor",
   "metadata": {},
   "source": [
    "#### SVD Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "material-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _embed(x, order=3, delay=1):  # credits to raphaelvallat\n",
    "    \"\"\"Time-delay embedding.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 1d-array\n",
    "        Time series, of shape (n_times)\n",
    "    order : int\n",
    "        Embedding dimension (order).\n",
    "    delay : int\n",
    "        Delay.\n",
    "    Returns\n",
    "    -------\n",
    "    embedded : ndarray\n",
    "        Embedded time-series, of shape (n_times - (order - 1) * delay, order)\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    if order * delay > N:\n",
    "        raise ValueError(\"Error: order * delay should be lower than x.size\")\n",
    "    if delay < 1:\n",
    "        raise ValueError(\"Delay has to be at least 1.\")\n",
    "    if order < 2:\n",
    "        raise ValueError(\"Order has to be at least 2.\")\n",
    "    Y = np.zeros((order, N - (order - 1) * delay))\n",
    "    for i in range(order):\n",
    "        Y[i] = x[i * delay:i * delay + Y.shape[1]]\n",
    "    return Y.T\n",
    "\n",
    "\n",
    "def svd_entropy(x, order=3, delay=1, normalize=False):\n",
    "    x = np.array(x)\n",
    "    mat = _embed(x, order=order, delay=delay)\n",
    "    W = np.linalg.svd(mat, compute_uv=False)\n",
    "    # Normalize the singular values\n",
    "    W /= sum(W)\n",
    "    svd_e = -np.multiply(W, np.log2(W)).sum()\n",
    "    if normalize:\n",
    "        svd_e /= np.log2(order)\n",
    "    return svd_e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-visitor",
   "metadata": {},
   "source": [
    "## Feature generation loop\n",
    "N.B. This loop assumes that data is stored in different folders for each patient and for train and test sets (this is how the data from Seer is provided so i made use of it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hindu-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(patient_number, data_path, is_training_data, save_to_disk = True):\n",
    "    \n",
    "    filenames = [f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "    filelist = [join(data_path, f) for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "\n",
    "\n",
    "    logging.debug(f'generated filelist of length {len(filelist)} for patient {patient_number}; is_training_data = {is_training_data}')\n",
    "    \n",
    "    counter = 0\n",
    "    for filename in tqdm(filenames):\n",
    "        \n",
    "        # Lists that will contain feature names and values, we will stack these to make X_train\n",
    "        index = []\n",
    "        features = []\n",
    "\n",
    "        #Load file & normalise\n",
    "        data = load_mat(join(data_path, filename))\n",
    "        data = preprocessing.scale(data, axis=1, with_std=True)\n",
    "        data_downsampled = scipy.signal.decimate(data, 5, zero_phase=True)\n",
    "        \n",
    "        logging.debug(f'starting feature generation file:{counter}')\n",
    "        \n",
    "        # ID features\n",
    "        index.append('Patient')\n",
    "        features.append(patient_number)\n",
    "        \n",
    "        index.append('filenumber')\n",
    "        features.append(filename[filename.find('_')+1:-6])\n",
    "        \n",
    "        #accross channels features on full data\n",
    "        correlation_matrix = np.corrcoef(data)\n",
    "        correlation_matrix = np.nan_to_num(correlation_matrix)\n",
    "        # take only values in upper triangle to avoid redundancy\n",
    "        triup_index = np.triu_indices(16, k=1)\n",
    "        for i, j in zip(triup_index[0], triup_index[1]):\n",
    "            features.append(correlation_matrix[i][j])\n",
    "            index.append(f'correlation_{i}-{j}')\n",
    "\n",
    "        eigenvals = np.linalg.eigvals(correlation_matrix)\n",
    "        eigenvals = np.nan_to_num(eigenvals)\n",
    "        eigenvals = np.real(eigenvals)\n",
    "        for i in CHANNELS:\n",
    "            features.append(eigenvals[i])\n",
    "            index.append(f'eigenval_{i}')\n",
    "            \n",
    "        # summed across all channels and frequencies\n",
    "        summed_energy = total_energy(data_downsampled)\n",
    "        features.append(summed_energy)\n",
    "        index.append('summed_energy')\n",
    "        \n",
    "        logging.debug('general features generated')\n",
    "        \n",
    "        #Per channel features\n",
    "        #TODO work on all channels in parrallel as one matrix, vectorise all of it\n",
    "        for c in CHANNELS:\n",
    "            \n",
    "            logging.debug(f'starting feature generation file:{counter}, channel:{c}')\n",
    "            \n",
    "            # Create necessary functions\n",
    "            data_channel = data_downsampled[c]\n",
    "            diff1 = np.diff(data_channel, n=1)\n",
    "            diff2 = np.diff(data_channel, n=2)\n",
    "\n",
    "            ## Simple features\n",
    "            std = np.std(data_channel)\n",
    "            features.append(std)\n",
    "            index.append(f'std_{c}')\n",
    "\n",
    "            skew = scipy.stats.skew(data_channel)\n",
    "            features.append(skew)\n",
    "            index.append(f'skew_{c}')\n",
    "\n",
    "            kurt = scipy.stats.kurtosis(data_channel)\n",
    "            features.append(kurt)\n",
    "            index.append(f'kurt_{c}')\n",
    "\n",
    "            zeros = zero_crossings(data_channel)\n",
    "            features.append(zeros)\n",
    "            index.append(f'zeros_{c}')\n",
    "            \n",
    "            logging.debug('simple features generated')\n",
    "\n",
    "            #RMS = np.sqrt(data_channel**2.mean())\n",
    "\n",
    "            ## Differential features\n",
    "            mobility = np.std(diff1)/np.std(data_channel)\n",
    "            features.append(mobility)\n",
    "            index.append(f'mobility_{c}')\n",
    "\n",
    "            complexity = (np.std(diff2) * np.std(diff2)) / np.std(diff1)\n",
    "            features.append(complexity)\n",
    "            index.append(f'complexity_{c}')\n",
    "\n",
    "            zeros_diff1 = zero_crossings(diff1)\n",
    "            features.append(zeros_diff1)\n",
    "            index.append(f'zeros_diff1_{c}')\n",
    "\n",
    "            zeros_diff2 = zero_crossings(diff2)\n",
    "            features.append(zeros_diff2)\n",
    "            index.append(f'zeros_diff2_{c}')\n",
    "\n",
    "            std_diff1 = np.std(diff1)\n",
    "            features.append(std_diff1)\n",
    "            index.append(f'std_diff1_{c}')\n",
    "\n",
    "            std_diff2 = np.std(diff2)\n",
    "            features.append(std_diff2)\n",
    "            index.append(f'std_diff2_{c}')\n",
    "            \n",
    "            logging.debug('differential features generated')\n",
    "\n",
    "            # Frequency features\n",
    "\n",
    "            ## Use welch method to approcimate energies per frequency subdivision\n",
    "            # From litterature the lowest frequencies of interest in a EEG is 0.5Hz so we need to keep our resolution at 0.25Hz hence a 4 second window cf.Nyquist\n",
    "            window = (SAMPLING_FREQUENCY / DOWNSAMPLING_RATIO) * 4\n",
    "            f, psd = scipy.signal.welch(data_channel, fs=80, nperseg=window)\n",
    "            psd = np.nan_to_num(psd)\n",
    "\n",
    "            ## Total summed energy\n",
    "            channel_energy = band_energy(f, psd, 0.1, 40)\n",
    "            features.append(channel_energy)\n",
    "            index.append(f'channel_{c}_energy')\n",
    "\n",
    "            ## Normalised summed energy\n",
    "            normalised_energy = channel_energy / summed_energy\n",
    "            features.append(normalised_energy)\n",
    "            index.append(f'normalised_energy_{c}')\n",
    "\n",
    "            ## Peak frequency\n",
    "            peak_frequency = f[np.argmax(psd)]\n",
    "            features.append(peak_frequency)\n",
    "            index.append(f'peak_frequency_{c}')\n",
    "\n",
    "            ## Normalised_summed energy per band\n",
    "            for k in range(len(BANDS)-1):\n",
    "                energy = band_energy(f, psd, BANDS[k], BANDS[k+1])\n",
    "                normalised_band_energy = energy / channel_energy\n",
    "                features.append(normalised_band_energy)\n",
    "                index.append(f'normalised_band_energy_{c}_{k}')\n",
    "                \n",
    "            logging.debug('lowres frequency features generated')\n",
    "\n",
    "            ## Spectral entropy\n",
    "            psd_norm = np.divide(psd, psd.sum())\n",
    "            spectral_entropy = -np.multiply(psd_norm, np.log2(psd_norm)).sum()\n",
    "            #spectral_entropy /= np.log2(psd_norm.size) #uncomment to normalise entropy\n",
    "            features.append(spectral_entropy)\n",
    "            index.append(f'spectral_entropy_{c}')\n",
    "\n",
    "            ## SVD entropy\n",
    "            entropy = svd_entropy(data_channel, order=3,\n",
    "                                  delay=1, normalize=False)\n",
    "            features.append(entropy)\n",
    "            index.append(f'svd_entropy_{c}')\n",
    "            \n",
    "            logging.debug('entropy features generated')\n",
    "\n",
    "            # Highres features : energy per frequency band in 1min segements\n",
    "            highres_channel_energy = highres_total_energy(data)\n",
    "            features.append(highres_channel_energy)\n",
    "            index.append(f'total_channel_energy_{c}')\n",
    "            \n",
    "            f, psd = scipy.signal.welch(data, fs=400, nperseg=SAMPLING_FREQUENCY*4)\n",
    "            psd = np.nan_to_num(psd)\n",
    "            full_psd_sum = psd.sum()/10  # for normalisation purposed\n",
    "            # TODO add band energy divided by full_psd_sum as feature\n",
    "            \n",
    "            # j allows us to iterate over 1min segments with 30s overlap\n",
    "            for j in range(19):\n",
    "                data_segment = data_channel[j*30*SAMPLING_FREQUENCY: (j+1)*30*SAMPLING_FREQUENCY]\n",
    "                f_segment, psd_segment = scipy.signal.welch(\n",
    "                    data_segment, fs=SAMPLING_FREQUENCY, nperseg=SAMPLING_FREQUENCY*4)\n",
    "                psd_segment = np.nan_to_num(psd_segment)\n",
    "\n",
    "                for k in range(len(HIGHRES_BANDS)-1):\n",
    "                    window_band_energy = psd_segment[(f_segment > HIGHRES_BANDS[k]) & (\n",
    "                        f_segment < HIGHRES_BANDS[k+1])].sum()\n",
    "                    features.append(window_band_energy)\n",
    "                    index.append(f'windowed_band_energy_{c}_{k}_{j}')\n",
    "                    normalised_window_band_energy = window_band_energy/full_psd_sum\n",
    "                    features.append(normalised_window_band_energy)\n",
    "                    index.append(f'normalised_window_band_energy_{c}_{k}_{j}')\n",
    "                    #TODO check if normalised feature is redundant\n",
    "                    \n",
    "            logging.debug('highres frequency features generated')\n",
    "            \n",
    "            #logging.debug(f'finished feature generation file:{counter}, channel:{c}')\n",
    "\n",
    "        # Save generated features to X_train\n",
    "        if counter == 0:\n",
    "            #X_train = np.zeros((1, len(features)))\n",
    "            X = np.array(features)\n",
    "            logging.debug('X created and updated')   \n",
    "        else:\n",
    "            X = np.vstack((X, np.array(features)))\n",
    "            logging.debug(f'features for file:{counter} added to array ; X.shape = {X.shape}')\n",
    "        \n",
    "        # Save label to y_train\n",
    "        if is_training_data:\n",
    "            label = filename[-5 : -4] #last char excluding .mat\n",
    "            if counter == 0:\n",
    "                y = np.array(label).astype('int')\n",
    "                logging.debug(' y created and updated')\n",
    "            else :\n",
    "                y = np.vstack((y, np.array(label))).astype('int')\n",
    "                logging.debug(f'label stacked onto y, y.shape = {y.shape}')\n",
    "        \n",
    "        counter += 1\n",
    "\n",
    "        #TODO add logging\n",
    "\n",
    "    # Save X_train to file before moving on to next patient data\n",
    "    X = np.nan_to_num(X)\n",
    "    y = np.nan_to_num(y)\n",
    "       \n",
    "    if is_training_data:\n",
    "        if save_to_disk:\n",
    "            np.save(join(FEATURE_SAVE_PATH, f'neurovista_X_train_pat{patient_number}.npy'), X)\n",
    "            np.save(join(FEATURE_SAVE_PATH, f'neurovista_y_train_pat{patient_number}.npy'), y)\n",
    "            logging.info('features and labels saved to disk')\n",
    "        return (X, y)\n",
    "    \n",
    "    if is_training_data == False:\n",
    "        if save_to_disk:\n",
    "            np.save(join(FEATURE_SAVE_PATH, f'neurovista_X_test_pat{patient_number}.npy'), X)\n",
    "            logging.info('features saved to disk')\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-collective",
   "metadata": {},
   "source": [
    "#### Loop over our patients and call feature generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "\n",
    "for p in [1, 2, 3]:  #[1, 2, 3]:  # iterating over patients 1, 2, 3\n",
    "\n",
    "    logging.info(f'Entering loop to generate train features for patient {p}')\n",
    "    patient_path = join(DATA_PATH, TRAIN_PATHS[p-1])\n",
    "    \n",
    "    data_dict[f'X_train_pat{p}'], data_dict[f'y_train_pat{p}'] = generate_features(patient_number = p,\n",
    "                                                                                   data_path = patient_path,\n",
    "                                                                                   save_to_disk = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-tobago",
   "metadata": {},
   "source": [
    "## EDA of the generated training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-costa",
   "metadata": {},
   "source": [
    "# II - Machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-outline",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-corporation",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "allied-active",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gijsb\\Anaconda3\\envs\\Epilepsy\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import sklearn.metrics\n",
    "import logging\n",
    "from sklearn.metrics import make_scorer\n",
    "from evolutionary_search import EvolutionaryAlgorithmSearchCV #\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-union",
   "metadata": {},
   "source": [
    "### Metrics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "specified-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(clf, X_test, y_test):\n",
    "    \"\"\"\n",
    "    return a dict containing auc, f1score, accuracy, balanced_accuracy, recall\n",
    "    \"\"\"\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, [1]]\n",
    "    \n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1score = f1_score(y_test, y_pred)\n",
    "    accuracy = sklearn.metrics.average_precision_score(y_test, y_pred)\n",
    "    balanced_accuracy = sklearn.metrics.balanced_accuracy_score(y_test, y_pred, adjusted = True)\n",
    "    recall = sklearn.metrics.recall_score(y_test, y_pred)\n",
    "    \n",
    "    metrics_dict = {}\n",
    "    metrics_dict['auc'] = auc\n",
    "    metrics_dict['f1score'] = f1score\n",
    "    metrics_dict['accuracy'] = accuracy\n",
    "    metrics_dict['balanced_accuracy'] = balanced_accuracy\n",
    "    metrics_dict['recall'] = recall\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-cambodia",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pat = {}\n",
    "y_pat = {}\n",
    "\n",
    "for p in [1,2,3]:\n",
    "    X_pat[f'pat{p}'] = np.load(f'neurovista_X_train_pat{p}.npy').astype('float32')\n",
    "    X_pat[f'pat{p}'] = np.nan_to_num(X_pat[f'pat{p}'])\n",
    "    \n",
    "    y_pat[f'pat{p}'] = np.load(f'neurovista_y_train_pat{p}.npy').astype('float32')\n",
    "    y_pat[f'pat{p}'] = np.nan_to_num(y_pat[f'pat{p}'])\n",
    "    \n",
    "logging.debug('X and y loaded into dictionary')\n",
    "\n",
    "# Assign data to train and test sets\n",
    "\n",
    "X = np.vstack(tuple(X_pat.values()))\n",
    "y = np.vstack(tuple(y_pat.values()))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
